{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saka-14B RAG System - Clean Version\n",
        "\n",
        "**Step-by-step setup for Arabic Mental Health QA**\n",
        "\n",
        "## Before You Start\n",
        "1. Select **A100 GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí A100\n",
        "2. Upload your data files (if not in repo):\n",
        "   - `knowldege_base/data/processed/articles_all.jsonl`\n",
        "   - `knowldege_base/data/processed/books_all_ragclean.jsonl`\n",
        "   - `knowldege_base/data/processed/shifaa_qa_pairs_all.jsonl`\n",
        "3. Run cells **sequentially** - don't skip steps!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q torch>=2.0.0 transformers>=4.40.0 accelerate>=0.20.0 bitsandbytes>=0.41.0\n",
        "%pip install -q sentence-transformers>=2.2.2 chromadb>=0.4.0 rank-bm25\n",
        "%pip install -q numpy pandas tqdm\n",
        "\n",
        "# Setup Python path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "if 'colab_files' in os.listdir('.'):\n",
        "    sys.path.insert(0, 'colab_files')\n",
        "elif 'knowldege_base' in os.listdir('.'):\n",
        "    sys.path.insert(0, '.')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Please upload colab_files/ or ensure knowldege_base/ is present\")\n",
        "\n",
        "print(f\"‚úÖ Python path: {sys.path[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Verify GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU! Select A100 GPU runtime.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build KB Chunks (One Time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from knowldege_base.rag_staging.kb_chunker import build_kb_chunks\n",
        "from knowldege_base.rag_staging.loader import describe_kb\n",
        "import os\n",
        "\n",
        "# Check what data files are available\n",
        "print(\"Checking available data files...\")\n",
        "processed_dir = 'knowldege_base/data/processed'\n",
        "if os.path.exists(processed_dir):\n",
        "    files = os.listdir(processed_dir)\n",
        "    print(f\"  Found {len(files)} files in {processed_dir}\")\n",
        "    for f in sorted(files):\n",
        "        if f.endswith('.jsonl'):\n",
        "            size = os.path.getsize(os.path.join(processed_dir, f)) / (1024*1024)\n",
        "            print(f\"    - {f} ({size:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Directory not found: {processed_dir}\")\n",
        "\n",
        "chunks_file = 'knowldege_base/data/processed/kb_chunks.jsonl'\n",
        "\n",
        "if os.path.exists(chunks_file):\n",
        "    print(f\"\\n‚úÖ KB chunks exist: {chunks_file}\")\n",
        "    print(\"   Skipping rebuild. Delete file to rebuild.\")\n",
        "    # Show summary\n",
        "    try:\n",
        "        stats = describe_kb()\n",
        "        print(f\"\\nüìä Current KB summary:\")\n",
        "        for k, v in stats.items():\n",
        "            print(f\"   {k}: {v}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   (Could not load summary: {e})\")\n",
        "else:\n",
        "    print(\"\\nBuilding KB chunks...\")\n",
        "    print(\"Note: Missing files will be skipped with warnings.\")\n",
        "    try:\n",
        "        build_kb_chunks(output_filename=\"kb_chunks.jsonl\")\n",
        "        print(f\"‚úÖ KB chunks built\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error building KB chunks: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"  1. Check that data files are uploaded:\")\n",
        "        print(\"     - articles_all.jsonl\")\n",
        "        print(\"     - books_all_ragclean.jsonl (optional)\")\n",
        "        print(\"     - shifaa_qa_pairs_all.jsonl (optional)\")\n",
        "        print(\"  2. Check file encoding (should be UTF-8)\")\n",
        "        print(\"  3. Check file paths are correct\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build Vector Database (One Time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from knowldege_base.rag_staging.vector_db import VectorDB\n",
        "import os\n",
        "import chromadb\n",
        "\n",
        "vector_db_path = 'knowldege_base/data/vector_db'\n",
        "\n",
        "if os.path.exists(vector_db_path):\n",
        "    try:\n",
        "        client = chromadb.PersistentClient(path=vector_db_path)\n",
        "        collections = client.list_collections()\n",
        "        if collections:\n",
        "            print(f\"‚úÖ Vector DB exists with {len(collections)} collection(s)\")\n",
        "            print(\"   Skipping rebuild. Delete directory to rebuild.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Rebuilding vector DB...\")\n",
        "            VectorDB.build(force_rebuild=True)\n",
        "            print(\"‚úÖ Vector DB rebuilt\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error: {e}, rebuilding...\")\n",
        "        VectorDB.build(force_rebuild=True)\n",
        "        print(\"‚úÖ Vector DB rebuilt\")\n",
        "else:\n",
        "    print(\"Building vector database...\")\n",
        "    VectorDB.build(force_rebuild=False)\n",
        "    print(f\"‚úÖ Vector DB built: {vector_db_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Choose Prompt Version\n",
        "\n",
        "**Version A: Original Prompt** (no filtering instructions)  \n",
        "**Version B: Filtering Prompt** (explicit instructions to avoid Quranic content)\n",
        "\n",
        "Set `PROMPT_VERSION` below to test which works better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose prompt version: 'original' or 'filtering'\n",
        "PROMPT_VERSION = 'original'  # Change to 'filtering' to test the other version\n",
        "\n",
        "print(f\"Using prompt version: {PROMPT_VERSION}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initialize RAG Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from knowldege_base.rag_staging.rag_qa import RAGQAPipeline\n",
        "import os\n",
        "\n",
        "# Set prompt version via environment variable\n",
        "os.environ['RAG_PROMPT_VERSION'] = PROMPT_VERSION\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Initializing Saka-14B RAG Pipeline...\")\n",
        "print(\"=\" * 80)\n",
        "print(\"This will:\")\n",
        "print(\"  1. Load Saka-14B model (~28GB) - 10-15 min first time\")\n",
        "print(\"  2. Load knowledge base and vector database\")\n",
        "print(\"  3. Ready to answer questions!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "rag = RAGQAPipeline.build(\n",
        "    model_name=\"Sakalti/Saka-14B\",\n",
        "    use_gpu=True,\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=False,\n",
        "    max_new_tokens=512,\n",
        "    download_to_local=False,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ RAG Pipeline Ready!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_query = \"ÿ£ÿπÿßŸÜŸä ŸÖŸÜ ÿßŸÑŸÇŸÑŸÇ ŸàÿßŸÑÿ™Ÿàÿ™ÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±ÿå ŸÖÿß ŸáŸä ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπŸáÿü\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUESTION:\")\n",
        "print(\"=\" * 80)\n",
        "print(test_query)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING ANSWER...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "result = rag.answer(\n",
        "    query=test_query,\n",
        "    top_k=5,\n",
        "    relevance_threshold=0.5,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ANSWER:\")\n",
        "print(\"=\" * 80)\n",
        "print(result.answer)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Used KB: {result.used_kb}\")\n",
        "print(f\"Top Score: {result.top_score:.4f}\")\n",
        "print(f\"Avg Top Score: {result.avg_top_score:.4f}\")\n",
        "print(f\"Answer Length: {len(result.answer)} chars\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
