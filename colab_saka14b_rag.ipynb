{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saka-14B RAG System on Google Colab\n",
        "\n",
        "**Efficient Setup for Arabic Mental Health QA**\n",
        "\n",
        "## Quick Start\n",
        "1. Select **A100 GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí A100\n",
        "2. Run all cells sequentially\n",
        "3. The notebook will:\n",
        "   - Install dependencies\n",
        "   - Build/load knowledge base once\n",
        "   - Build vector database once  \n",
        "   - Initialize RAG pipeline\n",
        "   - Ready to answer questions!\n",
        "\n",
        "## Important Notes\n",
        "- KB and Vector DB are built once and reused\n",
        "- No chunk filtering - generator is instructed to avoid Quranic content\n",
        "- Generator uses only Arabic language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q torch>=2.0.0 transformers>=4.40.0 accelerate>=0.20.0 bitsandbytes>=0.41.0\n",
        "%pip install -q sentence-transformers>=2.2.2 chromadb>=0.4.0 rank-bm25\n",
        "%pip install -q numpy pandas tqdm\n",
        "\n",
        "# Setup Python path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to Python path\n",
        "if 'colab_files' in os.listdir('.'):\n",
        "    sys.path.insert(0, 'colab_files')\n",
        "elif 'knowldege_base' in os.listdir('.'):\n",
        "    sys.path.insert(0, '.')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Please upload colab_files/ directory or ensure knowldege_base/ is in current directory\")\n",
        "\n",
        "print(f\"‚úÖ Python path configured: {sys.path[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Verify GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected! Please select A100 GPU runtime.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build Knowledge Base Chunks (One Time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build KB chunks from articles, books, and QA pairs\n",
        "# This only needs to run once - chunks are saved to disk\n",
        "\n",
        "from knowldege_base.rag_staging.kb_chunker import build_kb_chunks\n",
        "import os\n",
        "\n",
        "chunks_file = 'knowldege_base/data/processed/kb_chunks.jsonl'\n",
        "\n",
        "if os.path.exists(chunks_file):\n",
        "    print(f\"‚úÖ KB chunks already exist: {chunks_file}\")\n",
        "    print(\"   Skipping rebuild. Delete the file if you want to rebuild.\")\n",
        "else:\n",
        "    print(\"Building KB chunks...\")\n",
        "    print(\"This will read articles, books, and QA pairs and create chunks.\")\n",
        "    build_kb_chunks(output_filename=\"kb_chunks.jsonl\")\n",
        "    print(f\"‚úÖ KB chunks built: {chunks_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build Vector Database (One Time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vector database on disk\n",
        "# This only needs to run once - vector DB is saved to disk\n",
        "\n",
        "from knowldege_base.rag_staging.vector_db import VectorDB\n",
        "import os\n",
        "\n",
        "vector_db_path = 'knowldege_base/data/vector_db'\n",
        "\n",
        "if os.path.exists(vector_db_path):\n",
        "    try:\n",
        "        import chromadb\n",
        "        client = chromadb.PersistentClient(path=vector_db_path)\n",
        "        collections = client.list_collections()\n",
        "        if collections:\n",
        "            print(f\"‚úÖ Vector database exists with {len(collections)} collection(s)\")\n",
        "            print(f\"   Location: {vector_db_path}\")\n",
        "            print(\"   Skipping rebuild. Delete the directory if you want to rebuild.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Vector DB directory exists but no collections found. Rebuilding...\")\n",
        "            VectorDB.build(force_rebuild=True)\n",
        "            print(\"‚úÖ Vector database rebuilt!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error checking vector DB: {e}\")\n",
        "        print(\"   Rebuilding...\")\n",
        "        VectorDB.build(force_rebuild=True)\n",
        "        print(\"‚úÖ Vector database rebuilt!\")\n",
        "else:\n",
        "    print(\"Building vector database on disk...\")\n",
        "    print(\"This will take a few minutes but will persist across sessions.\")\n",
        "    VectorDB.build(force_rebuild=False)\n",
        "    print(f\"‚úÖ Vector database built: {vector_db_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Initialize RAG Pipeline with Saka-14B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from knowldege_base.rag_staging.rag_qa import RAGQAPipeline\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Initializing Saka-14B RAG Pipeline...\")\n",
        "print(\"=\" * 80)\n",
        "print(\"This will:\")\n",
        "print(\"  1. Load the Saka-14B model (~28GB) - may take 10-15 minutes on first run\")\n",
        "print(\"  2. Load the knowledge base (articles, books, QA pairs)\")\n",
        "print(\"  3. Load the vector database\")\n",
        "print(\"  4. Ready to answer questions!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize RAG with Saka-14B\n",
        "# On A100, we can use full precision\n",
        "rag = RAGQAPipeline.build(\n",
        "    model_name=\"Sakalti/Saka-14B\",\n",
        "    use_gpu=True,\n",
        "    load_in_4bit=False,  # A100 has enough VRAM\n",
        "    load_in_8bit=False,\n",
        "    max_new_tokens=512,  # Longer, more complete answers\n",
        "    download_to_local=False,  # Don't save to local (Colab disk is limited)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ RAG Pipeline Ready!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"The generator is configured to:\")\n",
        "print(\"  - Use only Arabic language\")\n",
        "print(\"  - Avoid Quranic verses (instructed in prompt)\")\n",
        "print(\"  - Focus on medical/psychological information\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5.5: Apply Critical Fixes (Device Mismatch & Quran Filtering)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive fix: Edit rag_qa.py and force reload\n",
        "import importlib\n",
        "import sys\n",
        "import re\n",
        "\n",
        "rag_qa_path = 'knowldege_base/rag_staging/rag_qa.py'\n",
        "\n",
        "# Read the file\n",
        "with open(rag_qa_path, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "fixes_applied = []\n",
        "\n",
        "# Fix 1: Device mismatch - ensure tokens are on model's device\n",
        "if 'model_device = next(model.parameters()).device' not in content:\n",
        "    # Find and replace the device assignment\n",
        "    pattern = r'(if self\\.device != \"cpu\":\\s+# Try to move to device, but if using device_map=\"auto\", model\\.generate handles it\\s+if not self\\._using_device_map_auto:\\s+prompt_tokens = prompt_tokens\\.to\\(self\\.device\\))'\n",
        "    replacement = '''if self.device != \"cpu\":\n",
        "                    # Get the device of the model's first parameter\n",
        "                    model_device = next(model.parameters()).device\n",
        "                    prompt_tokens = prompt_tokens.to(model_device)'''\n",
        "    \n",
        "    if re.search(pattern, content, re.MULTILINE | re.DOTALL):\n",
        "        content = re.sub(pattern, replacement, content, flags=re.MULTILINE | re.DOTALL)\n",
        "        fixes_applied.append(\"Device mismatch fix\")\n",
        "    else:\n",
        "        # Try alternative pattern\n",
        "        old_code = '''if self.device != \"cpu\":\n",
        "                    # Try to move to device, but if using device_map=\"auto\", model.generate handles it\n",
        "                    if not self._using_device_map_auto:\n",
        "                        prompt_tokens = prompt_tokens.to(self.device)'''\n",
        "        new_code = '''if self.device != \"cpu\":\n",
        "                    # Get the device of the model's first parameter\n",
        "                    model_device = next(model.parameters()).device\n",
        "                    prompt_tokens = prompt_tokens.to(model_device)'''\n",
        "        if old_code in content:\n",
        "            content = content.replace(old_code, new_code)\n",
        "            fixes_applied.append(\"Device mismatch fix\")\n",
        "\n",
        "# Fix 2: Add min_length for longer answers\n",
        "if 'min_length=min_length' not in content:\n",
        "    pattern = r'(generated_tokens = model\\.generate\\(\\s+prompt_tokens,\\s+max_new_tokens=self\\.max_new_tokens,)'\n",
        "    replacement = '''# Calculate minimum length to ensure substantial answers\n",
        "                    min_length = input_length + max(50, int(self.max_new_tokens * 0.4))\n",
        "                    \n",
        "                    generated_tokens = model.generate(\n",
        "                        prompt_tokens,\n",
        "                        max_new_tokens=self.max_new_tokens,\n",
        "                        min_length=min_length,'''\n",
        "    if re.search(pattern, content):\n",
        "        content = re.sub(pattern, replacement, content)\n",
        "        fixes_applied.append(\"Minimum length for longer answers\")\n",
        "\n",
        "# Fix 3: Add Quran filtering\n",
        "if 'is_quranic =' not in content or 'quran_verse_pattern' not in content:\n",
        "    quran_filter = '''        # CRITICAL: Filter out Quranic content to avoid misuse\n",
        "        import re\n",
        "        # Pattern 1: Verse markers like [ÿ∫ÿßŸÅÿ± Ÿ¶Ÿ†], [ÿßŸÑÿ®ŸÇÿ±ÿ© Ÿ¢Ÿ•Ÿ•], etc.\n",
        "        quran_verse_pattern = r'\\\\[[^\\\\]]*(?:ÿ∫ÿßŸÅÿ±|ÿßŸÑÿ®ŸÇÿ±ÿ©|ÿßŸÑŸÜÿ≥ÿßÿ°|ÿßŸÑŸÖÿßÿ¶ÿØÿ©|ÿßŸÑÿ£ŸÜÿπÿßŸÖ|ÿßŸÑÿ£ÿπÿ±ÿßŸÅ|ÿßŸÑÿ™Ÿàÿ®ÿ©|ŸäŸàŸÜÿ≥|ŸáŸàÿØ|ŸäŸàÿ≥ŸÅ|ÿ•ÿ®ÿ±ÿßŸáŸäŸÖ|ÿßŸÑŸÜÿ≠ŸÑ|ŸÖÿ±ŸäŸÖ|ÿ∑Ÿá|ÿßŸÑÿ£ŸÜÿ®Ÿäÿßÿ°|ÿßŸÑÿ≠ÿ¨|ÿßŸÑŸÜŸàÿ±|ÿßŸÑŸÅÿ±ŸÇÿßŸÜ|ÿßŸÑÿ¥ÿπÿ±ÿßÿ°|ÿßŸÑŸÜŸÖŸÑ|ÿßŸÑŸÇÿµÿµ|ÿßŸÑÿπŸÜŸÉÿ®Ÿàÿ™|ÿßŸÑÿ±ŸàŸÖ|ŸÑŸÇŸÖÿßŸÜ|ÿßŸÑÿ≥ÿ¨ÿØÿ©|ÿßŸÑÿ£ÿ≠ÿ≤ÿßÿ®|ÿ≥ÿ®ÿ£|ŸÅÿßÿ∑ÿ±|Ÿäÿ≥|ÿßŸÑÿµÿßŸÅÿßÿ™|ÿµ|ÿßŸÑÿ≤ŸÖÿ±|ŸÅÿµŸÑÿ™|ÿßŸÑÿ¥Ÿàÿ±Ÿâ|ÿßŸÑÿ≤ÿÆÿ±ŸÅ|ÿßŸÑÿØÿÆÿßŸÜ|ÿßŸÑÿ¨ÿßÿ´Ÿäÿ©|ÿßŸÑÿ£ÿ≠ŸÇÿßŸÅ|ŸÖÿ≠ŸÖÿØ|ÿßŸÑŸÅÿ™ÿ≠|ÿßŸÑÿ≠ÿ¨ÿ±ÿßÿ™|ŸÇ|ÿßŸÑÿ∞ÿßÿ±Ÿäÿßÿ™|ÿßŸÑÿ∑Ÿàÿ±|ÿßŸÑŸÜÿ¨ŸÖ|ÿßŸÑŸÇŸÖÿ±|ÿßŸÑÿ±ÿ≠ŸÖŸÜ|ÿßŸÑŸàÿßŸÇÿπÿ©|ÿßŸÑÿ≠ÿØŸäÿØ|ÿßŸÑŸÖÿ¨ÿßÿØŸÑÿ©|ÿßŸÑÿ≠ÿ¥ÿ±|ÿßŸÑŸÖŸÖÿ™ÿ≠ŸÜÿ©|ÿßŸÑÿµŸÅ|ÿßŸÑÿ¨ŸÖÿπÿ©|ÿßŸÑŸÖŸÜÿßŸÅŸÇŸàŸÜ|ÿßŸÑÿ™ÿ∫ÿßÿ®ŸÜ|ÿßŸÑÿ∑ŸÑÿßŸÇ|ÿßŸÑÿ™ÿ≠ÿ±ŸäŸÖ|ÿßŸÑŸÖŸÑŸÉ|ÿßŸÑŸÇŸÑŸÖ|ÿßŸÑÿ≠ÿßŸÇÿ©|ÿßŸÑŸÖÿπÿßÿ±ÿ¨|ŸÜŸàÿ≠|ÿßŸÑÿ¨ŸÜ|ÿßŸÑŸÖÿ≤ŸÖŸÑ|ÿßŸÑŸÖÿØÿ´ÿ±|ÿßŸÑŸÇŸäÿßŸÖÿ©|ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ|ÿßŸÑŸÖÿ±ÿ≥ŸÑÿßÿ™|ÿßŸÑŸÜÿ®ÿ£|ÿßŸÑŸÜÿßÿ≤ÿπÿßÿ™|ÿπÿ®ÿ≥|ÿßŸÑÿ™ŸÉŸàŸäÿ±|ÿßŸÑÿßŸÜŸÅÿ∑ÿßÿ±|ÿßŸÑŸÖÿ∑ŸÅŸÅŸäŸÜ|ÿßŸÑÿßŸÜÿ¥ŸÇÿßŸÇ|ÿßŸÑÿ®ÿ±Ÿàÿ¨|ÿßŸÑÿ∑ÿßÿ±ŸÇ|ÿßŸÑÿ£ÿπŸÑŸâ|ÿßŸÑÿ∫ÿßÿ¥Ÿäÿ©|ÿßŸÑŸÅÿ¨ÿ±|ÿßŸÑÿ®ŸÑÿØ|ÿßŸÑÿ¥ŸÖÿ≥|ÿßŸÑŸÑŸäŸÑ|ÿßŸÑÿ∂ÿ≠Ÿâ|ÿßŸÑÿ¥ÿ±ÿ≠|ÿßŸÑÿ™ŸäŸÜ|ÿßŸÑÿπŸÑŸÇ|ÿßŸÑŸÇÿØÿ±|ÿßŸÑÿ®ŸäŸÜÿ©|ÿßŸÑÿ≤ŸÑÿ≤ŸÑÿ©|ÿßŸÑÿπÿßÿØŸäÿßÿ™|ÿßŸÑŸÇÿßÿ±ÿπÿ©|ÿßŸÑÿ™ŸÉÿßÿ´ÿ±|ÿßŸÑÿπÿµÿ±|ÿßŸÑŸáŸÖÿ≤ÿ©|ÿßŸÑŸÅŸäŸÑ|ŸÇÿ±Ÿäÿ¥|ÿßŸÑŸÖÿßÿπŸàŸÜ|ÿßŸÑŸÉŸàÿ´ÿ±|ÿßŸÑŸÉÿßŸÅÿ±ŸàŸÜ|ÿßŸÑŸÜÿµÿ±|ÿßŸÑŸÖÿ≥ÿØ|ÿßŸÑÿ•ÿÆŸÑÿßÿµ|ÿßŸÑŸÅŸÑŸÇ|ÿßŸÑŸÜÿßÿ≥)[^\\\\]]*\\\\]'\n",
        "        # Pattern 2: Quranic verse markers Ô¥æ and Ô¥ø\n",
        "        quran_markers = r'[Ô¥æÔ¥ø]'\n",
        "        # Pattern 3: Common Quranic diacritics patterns\n",
        "        quran_diacritics = r'[ÿ®ŸéÿßÿØŸéÿ™Ÿê€å|ÿ≥Ÿé€åŸéÿØ€°ÿÆŸèŸÑŸèŸàŸÜŸé|ÿ¨ŸéŸáŸéŸÜŸéŸëŸÖŸé|ÿØŸéÿßÿÆŸêÿ±Ÿê€åŸÜŸé]'\n",
        "        \n",
        "        is_quranic = (\n",
        "            re.search(quran_verse_pattern, text, re.IGNORECASE) or\n",
        "            re.search(quran_markers, text) or\n",
        "            re.search(quran_diacritics, text)\n",
        "        )\n",
        "        \n",
        "        if is_quranic:\n",
        "            print(f\"‚ö† Skipping chunk {i} - contains Quranic content (to avoid misuse)\")\n",
        "            continue\n",
        "        \n",
        "        '''\n",
        "    # Insert before corruption check\n",
        "    if '# Skip chunks with obviously corrupted text' in content and 'is_quranic =' not in content:\n",
        "        content = content.replace(\n",
        "            '# Skip chunks with obviously corrupted text',\n",
        "            quran_filter + '# Skip chunks with obviously corrupted text',\n",
        "            1\n",
        "        )\n",
        "        fixes_applied.append(\"Quran content filtering\")\n",
        "\n",
        "# Write back if changes were made\n",
        "if fixes_applied:\n",
        "    with open(rag_qa_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "    print(f\"‚úÖ Applied fixes: {', '.join(fixes_applied)}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  All fixes already applied\")\n",
        "\n",
        "# Force reload ALL related modules\n",
        "modules_to_reload = [\n",
        "    'knowldege_base.rag_staging.rag_qa',\n",
        "    'knowldege_base.rag_staging',\n",
        "    'knowldege_base.rag_staging.hybrid_retriever',\n",
        "]\n",
        "\n",
        "for mod_name in modules_to_reload:\n",
        "    if mod_name in sys.modules:\n",
        "        importlib.reload(sys.modules[mod_name])\n",
        "        print(f\"‚úÖ Reloaded: {mod_name}\")\n",
        "\n",
        "# Also clear any cached bytecode\n",
        "import os\n",
        "import py_compile\n",
        "pyc_files = []\n",
        "for root, dirs, files in os.walk('knowldege_base'):\n",
        "    for file in files:\n",
        "        if file.endswith('.pyc') or file.endswith('__pycache__'):\n",
        "            pyc_files.append(os.path.join(root, file))\n",
        "\n",
        "for pyc in pyc_files:\n",
        "    try:\n",
        "        os.remove(pyc)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\n‚úÖ All modules reloaded! You MUST re-initialize RAG pipeline now:\")\n",
        "print(\"   rag = RAGQAPipeline.build(...)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5.6: Build Vector Database (Remove In-Memory Fallback)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vector database on disk (removes in-memory fallback)\n",
        "import os\n",
        "import shutil\n",
        "from knowldege_base.rag_staging.vector_db import VectorDB\n",
        "\n",
        "# Check if vector DB exists\n",
        "vector_db_path = 'knowldege_base/data/vector_db'\n",
        "if os.path.exists(vector_db_path):\n",
        "    print(f\"Found existing vector database at: {vector_db_path}\")\n",
        "    print(\"Checking if it's complete...\")\n",
        "    \n",
        "    # Check if ChromaDB collection exists\n",
        "    try:\n",
        "        import chromadb\n",
        "        client = chromadb.PersistentClient(path=vector_db_path)\n",
        "        collections = client.list_collections()\n",
        "        if collections:\n",
        "            print(f\"‚úÖ Vector database exists with {len(collections)} collection(s)\")\n",
        "            print(\"   Using existing database (no rebuild needed)\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Vector database directory exists but no collections found\")\n",
        "            print(\"   Rebuilding...\")\n",
        "            vector_db = VectorDB.build(force_rebuild=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error checking vector DB: {e}\")\n",
        "        print(\"   Rebuilding...\")\n",
        "        vector_db = VectorDB.build(force_rebuild=True)\n",
        "else:\n",
        "    print(\"Building vector database on disk...\")\n",
        "    print(\"This will take a few minutes but will persist across sessions.\")\n",
        "    vector_db = VectorDB.build(force_rebuild=False)\n",
        "\n",
        "print(\"\\n‚úÖ Vector database ready on disk!\")\n",
        "print(f\"   Location: {vector_db_path}\")\n",
        "print(\"   This will be used instead of in-memory fallback.\")\n",
        "print(\"\\nüí° When you initialize RAG, it will use this disk-based vector DB.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5.7: Diagnostic - Check Why Chunks Are Being Filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Check what's actually in the retrieved chunks\n",
        "from knowldege_base.rag_staging.hybrid_retriever import HybridKBRetriever\n",
        "\n",
        "# Initialize retriever\n",
        "retriever = HybridKBRetriever.build()\n",
        "\n",
        "# Test query\n",
        "test_query = \"ÿ£ÿπÿßŸÜŸä ŸÖŸÜ ÿßŸÑŸÇŸÑŸÇ ŸàÿßŸÑÿ™Ÿàÿ™ÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±ÿå ŸÖÿß ŸáŸä ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπŸáÿü\"\n",
        "\n",
        "# Get top 5 chunks\n",
        "results = retriever.search(test_query, top_k=5)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DIAGNOSTIC: Checking Retrieved Chunks\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, result in enumerate(results[:3], 1):  # Check first 3\n",
        "    chunk = result.get(\"chunk\", {})\n",
        "    text = chunk.get(\"text\", \"\")[:500]  # First 500 chars\n",
        "    kb_family = chunk.get(\"kb_family\", \"unknown\")\n",
        "    \n",
        "    print(f\"\\nChunk {i} ({kb_family}):\")\n",
        "    print(f\"Score: {result.get('score_hybrid', 0):.4f}\")\n",
        "    print(f\"Text preview: {text}...\")\n",
        "    \n",
        "    # Check for Quranic patterns\n",
        "    import re\n",
        "    has_verse_marker = bool(re.search(r'\\[[^\\]]*(?:ÿ∫ÿßŸÅÿ±|ÿßŸÑÿ®ŸÇÿ±ÿ©)[^\\]]*\\]', text, re.IGNORECASE))\n",
        "    has_markers = bool(re.search(r'[Ô¥æÔ¥ø]', text))\n",
        "    has_diacritics = bool(re.search(r'ÿ®ŸéÿßÿØŸéÿ™Ÿê€å|ÿ≥Ÿé€åŸéÿØ€°ÿÆŸèŸÑŸèŸàŸÜŸé|ÿ¨ŸéŸáŸéŸÜŸéŸëŸÖŸé|ÿØŸéÿßÿÆŸêÿ±Ÿê€åŸÜŸé', text))\n",
        "    \n",
        "    print(f\"  Has verse marker: {has_verse_marker}\")\n",
        "    print(f\"  Has Quran markers: {has_markers}\")\n",
        "    print(f\"  Has diacritics: {has_diacritics}\")\n",
        "    print(f\"  Would be filtered: {has_verse_marker or has_markers}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test Single Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query\n",
        "test_query = \"ÿ£ÿπÿßŸÜŸä ŸÖŸÜ ÿßŸÑŸÇŸÑŸÇ ŸàÿßŸÑÿ™Ÿàÿ™ÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±ÿå ŸÖÿß ŸáŸä ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπŸáÿü\"\n",
        "\n",
        "print(f\"Question: {test_query}\\n\")\n",
        "print(\"Generating answer...\\n\")\n",
        "\n",
        "result = rag.answer(\n",
        "    query=test_query,\n",
        "    top_k=5,\n",
        "    relevance_threshold=0.5,\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ANSWER:\")\n",
        "print(\"=\" * 80)\n",
        "print(result.answer)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Used KB: {result.used_kb}\")\n",
        "print(f\"Top Score: {result.top_score:.4f}\")\n",
        "print(f\"Avg Top Score: {result.avg_top_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Batch Processing (Optional)\n",
        "\n",
        "Process multiple questions from a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6.5: Verify QA Pairs Are Loaded (Diagnostic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Check if QA pairs are being loaded\n",
        "from knowldege_base.rag_staging import loader\n",
        "\n",
        "# Check file exists\n",
        "import os\n",
        "qa_path = \"/content/MentalQA_PLM/MentalQA_PLM/knowldege_base/data/processed/shifaa_qa_pairs_all.jsonl\"\n",
        "print(f\"QA file exists: {os.path.exists(qa_path)}\")\n",
        "print(f\"QA file path: {qa_path}\")\n",
        "\n",
        "# Try loading documents\n",
        "docs = loader.load_all_documents(\n",
        "    articles_filename=\"articles_all.jsonl\",\n",
        "    books_filename=\"books_all_ragclean.jsonl\",\n",
        "    qa_filename=\"shifaa_qa_pairs_all.jsonl\",\n",
        ")\n",
        "\n",
        "# Count by type\n",
        "article_count = sum(1 for d in docs if d.get(\"kb_family\") == \"article\")\n",
        "book_count = sum(1 for d in docs if d.get(\"kb_family\") == \"book\")\n",
        "qa_count = sum(1 for d in docs if d.get(\"kb_family\") == \"qa_pair\")\n",
        "\n",
        "print(f\"\\nLoaded documents:\")\n",
        "print(f\"  Articles: {article_count}\")\n",
        "print(f\"  Books: {book_count}\")\n",
        "print(f\"  QA Pairs: {qa_count}\")\n",
        "print(f\"  Total: {len(docs)}\")\n",
        "\n",
        "# Check a sample QA pair\n",
        "qa_samples = [d for d in docs if d.get(\"kb_family\") == \"qa_pair\"][:3]\n",
        "if qa_samples:\n",
        "    print(f\"\\nSample QA pair:\")\n",
        "    print(f\"  doc_id: {qa_samples[0].get('doc_id')}\")\n",
        "    print(f\"  has clean_text: {'clean_text' in qa_samples[0]}\")\n",
        "    print(f\"  clean_text length: {len(qa_samples[0].get('clean_text', ''))}\")\n",
        "    print(f\"  question: {qa_samples[0].get('question', '')[:100]}...\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No QA pairs found in loaded documents!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify QA chunks are actually in the kb_chunks.jsonl file\n",
        "import json\n",
        "\n",
        "chunks_file = \"/content/MentalQA_PLM/MentalQA_PLM/knowldege_base/data/processed/kb_chunks.jsonl\"\n",
        "\n",
        "qa_chunks_in_file = 0\n",
        "article_chunks_in_file = 0\n",
        "book_chunks_in_file = 0\n",
        "\n",
        "# Sample first 1000 chunks to check\n",
        "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 1000:  # Sample first 1000\n",
        "            break\n",
        "        chunk = json.loads(line)\n",
        "        kb_family = chunk.get(\"kb_family\", \"\")\n",
        "        if kb_family == \"qa_pair\":\n",
        "            qa_chunks_in_file += 1\n",
        "        elif kb_family == \"article\":\n",
        "            article_chunks_in_file += 1\n",
        "        elif kb_family == \"book\":\n",
        "            book_chunks_in_file += 1\n",
        "\n",
        "print(f\"Chunks in file (first 1000 sampled):\")\n",
        "print(f\"  QA Pairs: {qa_chunks_in_file}\")\n",
        "print(f\"  Articles: {article_chunks_in_file}\")\n",
        "print(f\"  Books: {book_chunks_in_file}\")\n",
        "\n",
        "# Count total QA chunks\n",
        "if qa_chunks_in_file == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  No QA chunks found! Checking all chunks...\")\n",
        "    total_qa = 0\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            chunk = json.loads(line)\n",
        "            if chunk.get(\"kb_family\") == \"qa_pair\":\n",
        "                total_qa += 1\n",
        "                if total_qa == 1:\n",
        "                    print(f\"\\nFound first QA chunk:\")\n",
        "                    print(f\"  chunk_id: {chunk.get('chunk_id')}\")\n",
        "                    print(f\"  doc_id: {chunk.get('parent_doc_id')}\")\n",
        "                    print(f\"  text preview: {chunk.get('text', '')[:200]}...\")\n",
        "    print(f\"\\nTotal QA chunks in file: {total_qa}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ QA chunks are present in the file!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Example: Process questions from a JSONL file\n",
        "def process_batch(input_file, output_file, rag_pipeline):\n",
        "    \"\"\"Process a batch of questions and save answers.\"\"\"\n",
        "    questions = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                questions.append(json.loads(line))\n",
        "    \n",
        "    results = []\n",
        "    for q_data in tqdm(questions, desc=\"Processing\"):\n",
        "        question = q_data.get('text', q_data.get('question', ''))\n",
        "        q_id = q_data.get('id', '')\n",
        "        \n",
        "        result = rag_pipeline.answer(\n",
        "            query=question,\n",
        "            top_k=5,\n",
        "            relevance_threshold=0.5,\n",
        "        )\n",
        "        \n",
        "        results.append({\n",
        "            'id': q_id,\n",
        "            'question': question,\n",
        "            'answer': result.answer,\n",
        "            'used_kb': result.used_kb,\n",
        "            'top_score': result.top_score,\n",
        "            'avg_top_score': result.avg_top_score,\n",
        "        })\n",
        "    \n",
        "    # Save results\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processed {len(results)} questions. Saved to {output_file}\")\n",
        "    return results\n",
        "\n",
        "# Uncomment to use:\n",
        "# process_batch('data/qtypes/test.jsonl', 'saka14b_answers.json', rag)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Download Results (Optional)\n",
        "\n",
        "Download generated answers from Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download results file\n",
        "# files.download('saka14b_answers.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Out of Memory (OOM)\n",
        "- Set `load_in_4bit=True` or `load_in_8bit=True`\n",
        "- Reduce `max_new_tokens`\n",
        "- Restart runtime and try again\n",
        "\n",
        "### Import Errors\n",
        "- Ensure `colab_files/` directory is uploaded\n",
        "- Check that `knowldege_base/rag_staging/__init__.py` exists\n",
        "- Verify Python path includes the directory\n",
        "\n",
        "### Vector DB Not Found\n",
        "- Upload `data/vector_db/` directory if you have it\n",
        "- Or rebuild it using the vector_db module\n",
        "\n",
        "### Slow Generation\n",
        "- Normal for 14B model (even on A100)\n",
        "- First generation is slower (model loading)\n",
        "- Subsequent queries should be faster\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
